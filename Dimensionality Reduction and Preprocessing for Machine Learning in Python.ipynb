{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64e9e947",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction and Preprocessing for Machine Learning in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49308a7",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55be78fb",
   "metadata": {},
   "source": [
    "Dimensionality reduction is a method of unsupervised learning. It refers to techniques for reducing the number of input variables in training data.\n",
    "\n",
    "However, on classification and regression predictive modeling datasets with supervised learning methods, it can be employed as a data transform pre-processing step for machine learning techniques.\n",
    "\n",
    "There are numerous dimensionality reduction techniques to choose from, and there is no one-size-fits-all solution. Instead, it's a good idea to experiment with a variety of dimensionality reduction techniques and their various setups.\n",
    "\n",
    "This article will show you how to use Python to fit and evaluate top dimensionality reduction techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0a26d9",
   "metadata": {},
   "source": [
    "Dimensionality reduction aims to reduce the number of dimensions in numerical input data while preserving the data's key relationships.\n",
    "\n",
    "There are numerous dimensionality reduction algorithms available, and there is no single ideal solution for all datasets.\n",
    "\n",
    "Here we get familiar to use the scikit-learn machine learning library to implement, fit, and assess top dimensionality reduction in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12196b6b",
   "metadata": {},
   "source": [
    "Hundreds, thousands, or even millions of input variables could be considered high-dimensionality.\n",
    "\n",
    "Fewer input dimensions usually imply fewer parameters or a simpler machine learning model structure, referred to as degrees of freedom. A model with too many degrees of freedom may overfit the training dataset and perform poorly on new data.\n",
    "\n",
    "Simple models that generalize effectively, as well as input data with few input variables, are preferred. This is especially true for linear models, where the number of inputs and the model's degrees of freedom are frequently linked.\n",
    "\n",
    "Prior to modeling, dimensionality reduction is a data preparation technique used on data. It could be done after cleaning and scaling the data and before training a prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58412341",
   "metadata": {},
   "source": [
    "As a result, any dimensionality reduction applied to training data must also be applied to new data, such as a test dataset, validation dataset, or data used to make a prediction with the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee103a9b",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a502a2d",
   "metadata": {},
   "source": [
    "Dimensionality reduction can be accomplished using a variety of algorithms.\n",
    "\n",
    "There are two types of methods: those based on linear algebra and those based on manifold learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fdd46e",
   "metadata": {},
   "source": [
    "Linear Algebra Methods\n",
    "- Dimensionality can be calculated using matrix factorization methods from the subject of linear algebra.\n",
    "- Some of the more popular methods include:\n",
    "\n",
    "    - Principal Components Analysis\n",
    "    - Singular Value Decomposition\n",
    "    - Non-Negative Matrix Factorization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9ac09f",
   "metadata": {},
   "source": [
    "Manifold Learning Methods\n",
    "- Manifold learning approaches aim to find a lower-dimensional projection of a high-dimensional input that captures the input data's most important qualities.\n",
    "- Some of the more popular methods include:\n",
    "    - Isomap Embedding\n",
    "    - Locally Linear Embedding\n",
    "    - Multidimensional Scaling\n",
    "    - Spectral Embedding\n",
    "    - t-distributed Stochastic Neighbor Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b399fe",
   "metadata": {},
   "source": [
    "Each method takes a different approach to the problem of detecting natural links in low-dimensional data.\n",
    "\n",
    "There is no single optimum dimensionality reduction algorithm, and there is no simple way to determine which algorithm is appropriate for your data without doing controlled tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4c4b8d",
   "metadata": {},
   "source": [
    "# Examples of Dimensionality Reduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e631a1",
   "metadata": {},
   "source": [
    "Classification Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98abf36c",
   "metadata": {},
   "source": [
    "To construct a test binary classification dataset, we'll utilize the make_classification() function.\n",
    "\n",
    "There will be 1,000 samples in the dataset, with 20 input attributes (10 relevant and 10 redundant). This gives each technique the chance to detect and eliminate unnecessary input features.\n",
    "\n",
    "The pseudorandom number generator's fixed random seed ensures that the same synthetic dataset is generated each time the code is run.\n",
    "\n",
    "The following is an example of how to create and summarize a synthetic classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4df7d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 20) (1000,)\n"
     ]
    }
   ],
   "source": [
    "# synthetic classification dataset\n",
    "from sklearn.datasets import make_classification\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=10, random_state=7)\n",
    "# summarize the dataset\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed63da52",
   "metadata": {},
   "source": [
    "It's a binary classification problem, and after each dimensionality reduction operation, we'll test a LogisticRegression model.\n",
    "\n",
    "The gold standard of repeated stratified 10-fold cross-validation will be used to assess the model. The mean and standard deviation classification accuracy will be presented for all folds and repeats.\n",
    "\n",
    "As a point of contrast, the sample below assesses the model on the raw dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2a66277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.824 (0.034)\n"
     ]
    }
   ],
   "source": [
    "# evaluate logistic regression model on raw data\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=10, random_state=7)\n",
    "# define the model\n",
    "model = LogisticRegression()\n",
    "# evaluate model\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be6c9fc",
   "metadata": {},
   "source": [
    "When the example is run on the raw dataset with all 20 columns, it achieves a classification accuracy of roughly 82.4 percent.\n",
    "\n",
    "Although this may not be attainable with all techniques, a successful dimensionality reduction transform on this data should result in a model that is more accurate than the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca499c7",
   "metadata": {},
   "source": [
    "After that, we may look at some instances of dimensionality reduction algorithms that have been used on this dataset.\n",
    "\n",
    "I've tried to tailor each approach to the dataset as much as possible. Wherever possible, each dimensionality reduction method will be adjusted to decrease the 20 input columns to ten.\n",
    "\n",
    "To integrate the data transform and model into an atomic unit that can be assessed using the cross-validation technique, we will use a Pipeline; for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c5d6bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e018ddfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the pipeline\n",
    "steps = [('pca', PCA(n_components=10)), ('m', LogisticRegression())]\n",
    "model = Pipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0aa515",
   "metadata": {},
   "source": [
    "Principal Component Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8523c70f",
   "metadata": {},
   "source": [
    "The most prevalent technique for dimensionality reduction with dense data is Principal Component Analysis, or PCA (few zero values)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29033a86",
   "metadata": {},
   "source": [
    "The following is a complete example of evaluating a model using PCA dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57dfa2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.824 (0.034)\n"
     ]
    }
   ],
   "source": [
    "# evaluate pca with logistic regression algorithm for classification\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=10, random_state=7)\n",
    "# define the pipeline\n",
    "steps = [('pca', PCA(n_components=10)), ('m', LogisticRegression())]\n",
    "model = Pipeline(steps=steps)\n",
    "# evaluate model\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168d0530",
   "metadata": {},
   "source": [
    "The example uses dimensionality reduction and a logistic regression prediction model to evaluate the modeling pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbdaf7e",
   "metadata": {},
   "source": [
    "# Preprocessing for Machine Learning in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5bc615",
   "metadata": {},
   "source": [
    "In Machine Learning, data preparation is a critical step that improves data quality and facilitates the extraction of relevant insights from the data. In Machine Learning, data preprocessing refers to the process of cleaning and organizing raw data in order to make it appropriate for creating and training Machine Learning models. In simple terms, data preprocessing is a data mining approach used in Machine Learning that turns raw data into a legible and intelligible format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864b1f69",
   "metadata": {},
   "source": [
    "Data preparation is the first stage in developing a Machine Learning model, and it marks the start of the process. Real-world data is frequently partial, inconsistent, erroneous (due to errors or outliers), and lacking in exact attribute values and trends. This is where data preparation comes into play: it cleans, formats, and organizes raw data, making it ready for Machine Learning models to use. Let's have a look at the different stages of data preprocessing in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f61bc34",
   "metadata": {},
   "source": [
    "Steps in Data Preprocessing in Machine Learning\n",
    "1. Acquire the dataset\n",
    "2. Import all the crucial libraries\n",
    "3. Import the dataset\n",
    "4. Identifying and handling the missing values\n",
    "5. Encoding the categorical data\n",
    "6. Splitting the dataset\n",
    "7. Feature scaling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99841f13",
   "metadata": {},
   "source": [
    "# 1. Acquire the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d94ea4",
   "metadata": {},
   "source": [
    "The initial stage in machine learning data preparation is to acquire the dataset. You must first obtain the required dataset before you can create and develop Machine Learning models. This dataset will be made up of data acquired from a variety of sources, which will be merged in a logical manner to make a dataset. The formats of datasets vary depending on the use case. A corporate dataset, for example, will be very different from a medical dataset. A medical dataset will include healthcare-related data, whereas a business dataset will comprise important industrial and business data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e470ac4",
   "metadata": {},
   "source": [
    "Datasets can be downloaded from numerous online sites, including https://www.kaggle.com/uciml/datasets You may also generate a dataset by using several Python APIs to collect data. Once you've completed the dataset, save it as a CSV, HTML, or XLSX file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e705880",
   "metadata": {},
   "source": [
    "# 2. Import all the crucial libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a766921",
   "metadata": {},
   "source": [
    "We'll show you how to import Python libraries for data preprocessing in Machine Learning because Python is the most widely used and recommended library by Data Scientists all over the world. More information about Python libraries for Data Science may be found here. Specific data pretreatment activities can be performed using the specified Python libraries. The second stage in machine learning data preprocessing is to import all of the necessary libraries. The following are the three main Python libraries used for data preprocessing in Machine Learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3062ae",
   "metadata": {},
   "source": [
    "- NumPy - NumPy is a Python library that allows you to perform scientific calculations. As a result, it's used to add any form of mathematical operation to the code. Large multidimensional arrays and matrices can also be used in NumPy programs.\n",
    "\n",
    "- Pandas — Pandas is a fantastic open-source Python data manipulation and analysis package. It's a popular tool for importing and maintaining datasets. It includes Python data structures and data analysis tools that are fast and simple to use.\n",
    "\n",
    "- Matplotlib - Matplotlib is a 2D charting toolkit for Python that may be used to create any style of chart. It can produce publication-quality numbers in a variety of hard copy and interactive formats across several platforms (IPython shells, Jupyter notebook, web application servers, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607be2c8",
   "metadata": {},
   "source": [
    "# 3. Import the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e52ec87",
   "metadata": {},
   "source": [
    "In this stage, you'll import the datasets you've obtained for your machine learning project. One of the most critical processes in data preprocessing in machine learning is importing the dataset. You must, however, establish the current directory as the working directory before you can import the dataset/s. In Spyder IDE, you can set the working directory in three easy steps:\n",
    "\n",
    "1. Save your Python script in the same directory as the dataset.\n",
    "2. In Spyder IDE, go to File Explorer and choose the needed directory.\n",
    "3. To run the file, press F5 or select Run from the File menu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1881eed",
   "metadata": {},
   "source": [
    "# 4. Identifying and handling the missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aab523",
   "metadata": {},
   "source": [
    "It's important to identify and manage missing values effectively during data preparation; if you don't, you risk drawing incorrect and erroneous conclusions and inferences from the data. Needless to say, this will sabotage your machine learning effort.\n",
    "\n",
    "In general, there are two approaches to dealing with missing data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2962ff7f",
   "metadata": {},
   "source": [
    "- Deleting a particular row – This method is used to delete a specific row that has a null value for a feature or a specific column with more than 75% of the data missing. However, because this strategy isn't 100 percent effective, it's best to utilize it only when the dataset includes enough samples. You must guarantee that no bias remains after the data has been deleted.\n",
    "\n",
    "\n",
    "- Calculating the mean – This method is useful for features having numeric data like age, salary, year, etc. Here, you can calculate the mean, median, or mode of a particular feature or column or row that contains a missing value and replace the result for the missing value. This method can add variance to the dataset, and any loss of data can be efficiently negated. Hence, it yields better results compared to the first method (omission of rows/columns). Another way of approximation is through the deviation of neighbouring values. However, this works best for linear data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cce39e",
   "metadata": {},
   "source": [
    "# 5. Encoding the categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355525f1",
   "metadata": {},
   "source": [
    "Categorical data is information that is divided into distinct categories within a dataset. There are two categorical variables in the dataset described above: nation and purchased.\n",
    "\n",
    "The majority of Machine Learning models are built on mathematical formulae. As a result, it's easy to see how keeping category data in the equation can cause problems, as you'll only need numbers in the calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07d1b65",
   "metadata": {},
   "source": [
    "# 6. Splitting the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed41178",
   "metadata": {},
   "source": [
    "The next stage in machine learning data preprocessing is to split the dataset. Every dataset for a Machine Learning model should be divided into two sets: training and test.\n",
    "The training set refers to the portion of a dataset that is utilized to train a machine learning model. You are already aware of the result in this case. On the other hand, a test set is a subset of the dataset that is used to test the machine learning model. The test set is used by the ML model to predict outcomes.\n",
    "\n",
    "The dataset is usually divided into 70:30 or 80:20 ratios. This means that you either use 70% or 80% of the data to train the model, while leaving out the remaining 30% or 20%. The method for separating the dataset differs depending on its shape and size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e465043b",
   "metadata": {},
   "source": [
    "To split the dataset, you have to write the following line of code – \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0898fdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "x_train, x_test, y_train, y_test= train_test_split(x, y, test_size= 0.2, random_state=0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95673a5",
   "metadata": {},
   "source": [
    "Here, the first line splits the arrays of the dataset into random train and test subsets. The second line of code includes four variables:\n",
    "\n",
    "x_train – features for the training data\n",
    "\n",
    "x_test – features for the test data\n",
    "\n",
    "y_train – dependent variables for training data\n",
    "\n",
    "y_test – independent variable for testing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef3d7f4",
   "metadata": {},
   "source": [
    "As a result, the train test split() function has four parameters, two of which are for data arrays. The size of the test set is specified by the test size function. This indicates the dividing ratio between the training and test sets and could be.5,.3, or.2. The final parameter, \"random state,\" specifies the seed for a random generator, ensuring that the result is consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81331dc7",
   "metadata": {},
   "source": [
    "# 7. Feature scaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19294d21",
   "metadata": {},
   "source": [
    "In Machine Learning, feature scaling signifies the end of data preprocessing. It's a technique for keeping a dataset's independent variables inside a certain range. To put it another way, feature scaling narrows the range of variables so that you can compare them on a level playing field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b91a53",
   "metadata": {},
   "source": [
    "Most ML models are based on Euclidean Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d37985",
   "metadata": {},
   "source": [
    "You can perform feature scaling in Machine Learning in two ways:\n",
    "- Standardization\n",
    "\n",
    "- Normalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc578a7",
   "metadata": {},
   "source": [
    "The standardization approach will be used for our dataset. To accomplish so, we'll use the following piece of code to import the sci-kit-learn library's StandardScaler class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650652b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf33e9f2",
   "metadata": {},
   "source": [
    "The next step will be to create the object of StandardScaler class for independent variables. After this, you can fit and transform the training dataset using the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96334a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "st_x= StandardScaler()  \n",
    "x_train= st_x.fit_transform(x_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4ecdde",
   "metadata": {},
   "source": [
    "You can use the transform() function directly on the test dataset (you don't need to use the fit transform() function because it's been done in the training set). The code will look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5872b02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test= st_x.transform(x_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7f09455",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '’' (U+2019) (3722602021.py, line 33)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [15]\u001b[0;36m\u001b[0m\n\u001b[0;31m    imputer= Imputer(missing_values =’NaN’, strategy=’mean’, axis = 0)\u001b[0m\n\u001b[0m                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '’' (U+2019)\n"
     ]
    }
   ],
   "source": [
    "# importing libraries  \n",
    "\n",
    "import numpy as nm  \n",
    "\n",
    "import matplotlib.pyplot as mtp  \n",
    "\n",
    "import pandas as pd  \n",
    "\n",
    "  \n",
    "\n",
    "#importing datasets  \n",
    "\n",
    "data_set= pd.read_csv(‘Dataset.csv’)  \n",
    "\n",
    "  \n",
    "\n",
    "#Extracting Independent Variable  \n",
    "\n",
    "x= data_set.iloc[:, :-1].values  \n",
    "\n",
    "  \n",
    "\n",
    "#Extracting Dependent variable  \n",
    "\n",
    "y= data_set.iloc[:, 3].values  \n",
    "\n",
    "  \n",
    "\n",
    "#handling missing data(Replacing missing data with the mean value)  \n",
    "\n",
    "from sklearn.preprocessing import Imputer  \n",
    "\n",
    "imputer= Imputer(missing_values =’NaN’, strategy=’mean’, axis = 0)  \n",
    "\n",
    "  \n",
    "\n",
    "#Fitting imputer object to the independent varibles x.   \n",
    "\n",
    "imputerimputer= imputer.fit(x[:, 1:3])  \n",
    "\n",
    "  \n",
    "\n",
    "#Replacing missing data with the calculated mean value  \n",
    "\n",
    "x[:, 1:3]= imputer.transform(x[:, 1:3])  \n",
    "\n",
    "  \n",
    "\n",
    "#for Country Variable  \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder  \n",
    "\n",
    "label_encoder_x= LabelEncoder()  \n",
    "\n",
    "x[:, 0]= label_encoder_x.fit_transform(x[:, 0])  \n",
    "\n",
    "  \n",
    "\n",
    "#Encoding for dummy variables  \n",
    "\n",
    "onehot_encoder= OneHotEncoder(categorical_features= [0])    \n",
    "\n",
    "x= onehot_encoder.fit_transform(x).toarray()  \n",
    "\n",
    "  \n",
    "\n",
    "#encoding for purchased variable  \n",
    "\n",
    "labelencoder_y= LabelEncoder()  \n",
    "\n",
    "y= labelencoder_y.fit_transform(y)  \n",
    "\n",
    "  \n",
    "\n",
    "# Splitting the dataset into training and test set.  \n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "\n",
    "x_train, x_test, y_train, y_test= train_test_split(x, y, test_size= 0.2, random_state=0)  \n",
    "\n",
    "  \n",
    "\n",
    "#Feature Scaling of datasets  \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "st_x= StandardScaler()  \n",
    "\n",
    "x_train= st_x.fit_transform(x_train)  \n",
    "\n",
    "x_test= st_x.transform(x_test)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73154469",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
